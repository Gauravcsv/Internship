{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035fc303",
   "metadata": {},
   "source": [
    "# Question 1-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057767a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1','h2','h3','h4','h5','h6'])\n",
    "print('List all the header tabs :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a106e9",
   "metadata": {},
   "source": [
    "# Question 2-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.imdb.com/chart/top/'\n",
    "page = requests.get(url)\n",
    "page\n",
    "\n",
    "page.content\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "print(soup.prettify)\n",
    "scraped_movies = soup.find_all('td',class_='titleColumn')\n",
    "scraped_movies\n",
    "movies = []\n",
    "for movie in scraped_movies:\n",
    "    movies.append(movie.text.replace('\\n',''))\n",
    "movies\n",
    "len(movies)\n",
    "scraped_ratings = soup.find_all('td',class_='ratingColumn imdbRating')\n",
    "scraped_ratings\n",
    "ratings = []\n",
    "for rating in scraped_ratings:\n",
    "    ratings.append(rating.text.replace('\\n',''))\n",
    "ratings\n",
    "len(ratings)\n",
    "realesed_years = soup.find_all('span',class_='secondaryInfo')\n",
    "realesed_years\n",
    "years = []\n",
    "for year in realesed_years:\n",
    "    years.append(year.text.replace('\\n',''))\n",
    "years\n",
    "len(years)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['Movie Name'] = movies\n",
    "data['Rating'] = ratings\n",
    "data['Year of Release'] = years\n",
    "data\n",
    "\n",
    "df = data.drop(data.index[100:])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb1cd6",
   "metadata": {},
   "source": [
    "# Question 3-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af24cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "page = requests.get('https://www.imdb.com/india/top-rated-indian-movies/')\n",
    "page\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "print(soup.prettify)\n",
    "indian_movie_data = soup.find_all('td',class_='titleColumn')\n",
    "indian_movie_data\n",
    "movies = []\n",
    "for movie in indian_movie_data:\n",
    "    movies.append(movie.text.replace('\\n','').strip())\n",
    "movies\n",
    "len(movies)\n",
    "scraped_ratings = soup.find_all('td',class_='ratingColumn imdbRating')\n",
    "scraped_ratings\n",
    "ratings = []\n",
    "for rating in scraped_ratings:\n",
    "    ratings.append(rating.text.replace('\\n',''))     \n",
    "ratings\n",
    "len(ratings)\n",
    "year_of_release = soup.find_all('span',class_='secondaryInfo')\n",
    "year_of_release\n",
    "years = []\n",
    "for year in year_of_release:\n",
    "    years.append(year.text.replace('\\n','')) \n",
    "years\n",
    "len(years)\n",
    "data = pd.DataFrame()\n",
    "data['Movie Name'] = movies\n",
    "data['Rating'] = ratings\n",
    "data['Year of Release'] = years\n",
    "data\n",
    "\n",
    "df = data.drop(data.index[100:])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a4974",
   "metadata": {},
   "source": [
    "# Quesion 4-(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e718c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "page\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "print(soup.prettify)\n",
    "scraped_countries = soup.find_all('span',class_='u-hide-phablet') \n",
    "scraped_countries\n",
    "countries = []\n",
    "for country in scraped_countries:\n",
    "    countries.append(country.text.replace('\\n',''))\n",
    "countries\n",
    "len(countries) \n",
    "no_of_matches = []\n",
    "match = soup.find_all('td',class_='rankings-block__banner--matches')\n",
    "for i in match:\n",
    "    no_of_matches.append(i.text)\n",
    "no_of_matches\n",
    "complete_info = []\n",
    "match_1 = soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "for i in match_1:\n",
    "    complete_info.append(i.text)\n",
    "complete_info \n",
    "\n",
    "for i in range(0,len(complete_info),2):\n",
    "    no_of_matches.append(complete_info[i])\n",
    "no_of_matches\n",
    "len(no_of_matches)\n",
    "\n",
    "no_of_point = []\n",
    "point = soup.find_all('td',class_='rankings-block__banner--points')\n",
    "for i in point:\n",
    "    no_of_point.append(i.text)\n",
    "no_of_point\n",
    "complete_info = []\n",
    "point_1 = soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "for i in point_1:\n",
    "    complete_info.append(i.text)\n",
    "complete_info \n",
    "\n",
    "for i in range(1,len(complete_info),2):\n",
    "    no_of_point.append(complete_info[i])\n",
    "no_of_point\n",
    "len(no_of_point)\n",
    "ratings = []\n",
    "rating = soup.find_all('td',class_='rankings-block__banner--rating u-text-right')\n",
    "for i in rating:\n",
    "    ratings.append(i.text.replace('\\n',''))\n",
    "ratings\n",
    "rating_1 = soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "for i in rating_1:\n",
    "    ratings.append(i.text.replace('\\n',''))\n",
    "ratings\n",
    "len(ratings)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['Matches'] = no_of_matches\n",
    "data['Points'] = no_of_point\n",
    "data['Ratings'] = ratings\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc6487",
   "metadata": {},
   "source": [
    "# Question 4-(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting')\n",
    "page\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    " \n",
    "batsmen = []\n",
    "batsman = soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "for i in batsman:\n",
    "    batsmen.append(i.text)\n",
    "batsmen\n",
    "batsman = soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "for i in batsman:\n",
    "    batsmen.append(i.text.replace('\\n',''))\n",
    "batsmen\n",
    "len(batsmen)\n",
    "\n",
    "ratings = []\n",
    "rating = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "for i in rating:\n",
    "    ratings.append(i.text)\n",
    "ratings\n",
    "rating_1 = soup.find_all('td',class_='table-body__cell rating')\n",
    "for i in rating_1:\n",
    "    ratings.append(i.text.replace('\\n',''))\n",
    "ratings\n",
    "len(ratings)\n",
    "\n",
    "teams = []\n",
    "team = soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "for i in team:\n",
    "    teams.append(i.text.replace('\\n',''))\n",
    "teams\n",
    "team_1 = soup.find_all('span',class_='table-body__logo-text')\n",
    "for i in team_1:\n",
    "    teams.append(i.text.replace('\\n',''))\n",
    "teams\n",
    "len(teams)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['Batsmen'] = batsmen\n",
    "data['Teams'] = teams\n",
    "data['Ratings'] = ratings\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f7cab",
   "metadata": {},
   "source": [
    "# Question 4-(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c376d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')\n",
    "page\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "bowlers = []\n",
    "bowler = soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "for i in bowler:\n",
    "    bowlers.append(i.text)\n",
    "bowlers\n",
    "bowler_1 = soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "for i in bowler_1:\n",
    "    bowlers.append(i.text.replace('\\n',''))\n",
    "bowlers\n",
    "len(bowlers)\n",
    "\n",
    "teams = []\n",
    "team = soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "for i in team:\n",
    "    teams.append(i.text.replace('\\n',''))\n",
    "teams\n",
    "team_1 = soup.find_all('span',class_='table-body__logo-text')\n",
    "for i in team_1:\n",
    "    teams.append(i.text.replace('\\n',''))\n",
    "teams\n",
    "len(teams)\n",
    "\n",
    "ratings = []\n",
    "rating = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "for i in rating:\n",
    "    ratings.append(i.text)\n",
    "ratings\n",
    "rating_1 = soup.find_all('td',class_='table-body__cell rating')\n",
    "for i in rating_1:\n",
    "    ratings.append(i.text.replace('\\n',''))\n",
    "ratings\n",
    "len(ratings)\n",
    "\n",
    "records = []\n",
    "record = soup.find_all('div',class_='rankings-block__career-best')\n",
    "for i in record:\n",
    "    records.append(i.text.replace('\\n',''))\n",
    "records\n",
    "record_1 = soup.find_all('td',class_='table-body__cell u-text-right u-hide-phablet')\n",
    "for i in record_1:\n",
    "    records.append(i.text.replace('\\n',''))\n",
    "records\n",
    "len(records)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['Bowlers'] = bowlers\n",
    "data['Teams'] = teams\n",
    "data['Ratings'] = ratings\n",
    "data['Records'] = records\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5bce9",
   "metadata": {},
   "source": [
    "# Question 5-(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df97bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "page\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "teams_1 = []\n",
    "team_ = soup.find_all('td',class_='rankings-block__banner--team-name')\n",
    "for i in team_:\n",
    "    teams_1.append(i.text.replace('\\n',''))\n",
    "teams_1\n",
    "team_1 = soup.find_all('td',class_='table-body__cell rankings-table__team')\n",
    "for i in team_1:\n",
    "    teams_1.append(i.text.replace('\\n',''))\n",
    "teams_1\n",
    "len(teams_1)\n",
    "\n",
    "no_of_matches = []\n",
    "matche = soup.find_all('td',class_='rankings-block__banner--matches')\n",
    "for i in matche:\n",
    "    no_of_matches.append(i.text.replace('\\n',''))\n",
    "no_of_matches\n",
    "complete_info = []\n",
    "match_1 = soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "for i in match_1:\n",
    "    complete_info.append(i.text)\n",
    "complete_info\n",
    "\n",
    "for i in range(0,len(complete_info),2):\n",
    "    no_of_matches.append(complete_info[i])\n",
    "no_of_matches\n",
    "len(no_of_matches)\n",
    "\n",
    "no_of_point = []\n",
    "point = soup.find_all('td',class_='rankings-block__banner--points')\n",
    "for i in point:\n",
    "    no_of_point.append(i.text)\n",
    "no_of_point\n",
    "complete_info = []\n",
    "point_1 = soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "for i in point_1:\n",
    "    complete_info.append(i.text)\n",
    "complete_info \n",
    "\n",
    "for i in range(1,len(complete_info),2):\n",
    "    no_of_point.append(complete_info[i])\n",
    "no_of_point\n",
    "len(no_of_point)\n",
    "\n",
    "ratings = []\n",
    "rating = soup.find_all('td',class_='rankings-block__banner--rating u-text-right')\n",
    "for i in rating:\n",
    "    ratings.append(i.text.replace('\\n',''))\n",
    "ratings\n",
    "rating_1 = soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "for i in rating_1:\n",
    "    ratings.append(i.text.replace('\\n',''))\n",
    "ratings\n",
    "len(ratings)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['Teams'] = teams_1\n",
    "data['No of matches'] = no_of_matches\n",
    "data['Points'] = no_of_point\n",
    "data['Ratings'] = ratings\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96b19a",
   "metadata": {},
   "source": [
    "# Question 5-(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd180c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')\n",
    "page\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "batswomen = []\n",
    "batswoman = soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "for i in batswoman:\n",
    "    batswomen.append(i.text)\n",
    "batswomen\n",
    "batswoman = soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "for i in batswoman:\n",
    "    batswomen.append(i.text.replace('\\n',''))\n",
    "batswomen\n",
    "len(batswomen)\n",
    "\n",
    "ratings = []\n",
    "rating = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "for i in rating:\n",
    "    ratings.append(i.text)\n",
    "ratings\n",
    "rating_1 = soup.find_all('td',class_='table-body__cell rating')\n",
    "for i in rating_1:\n",
    "    ratings.append(i.text.replace('\\n',''))\n",
    "ratings\n",
    "len(ratings)\n",
    "\n",
    "teams = []\n",
    "team = soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "for i in team:\n",
    "    teams.append(i.text.replace('\\n',''))\n",
    "teams\n",
    "team_1 = soup.find_all('span',class_='table-body__logo-text')\n",
    "for i in team_1:\n",
    "    teams.append(i.text.replace('\\n',''))\n",
    "teams\n",
    "len(teams)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['Batswomen'] = batswomen\n",
    "data['Teams'] = teams\n",
    "data['Ratings'] = ratings\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d8b57f",
   "metadata": {},
   "source": [
    "# Question 5-(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c052046",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder')\n",
    "page\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "all_rounder_1 = []\n",
    "all_rounder = soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "for i in all_rounder:\n",
    "    all_rounder_1.append(i.text)\n",
    "all_rounder_1\n",
    "all_rounder_ = soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "for i in all_rounder_:\n",
    "    all_rounder_1.append(i.text)\n",
    "all_rounder_1\n",
    "len(all_rounder_1)\n",
    "\n",
    "teams = []\n",
    "team = soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "for i in team:\n",
    "    teams.append(i.text.replace('\\n',''))\n",
    "teams\n",
    "team_1 = soup.find_all('span',class_='table-body__logo-text')\n",
    "for i in team_1:\n",
    "    teams.append(i.text.replace('\\n',''))\n",
    "teams\n",
    "len(teams)\n",
    "\n",
    "ratings = []\n",
    "rating = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "for i in rating:\n",
    "    ratings.append(i.text)\n",
    "ratings\n",
    "rating_1 = soup.find_all('td',class_='table-body__cell rating')\n",
    "for i in rating_1:\n",
    "    ratings.append(i.text.replace('\\n',''))\n",
    "ratings\n",
    "len(ratings)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['Name of all_rounder'] = all_rounder_1\n",
    "data['Teams'] = teams\n",
    "data['Ratings'] = ratings\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdfbcef",
   "metadata": {},
   "source": [
    "# Question 6-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d11f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "page11=requests.get(\"https://coreyms.com/\")\n",
    "page11\n",
    "\n",
    "soup_coreym=BeautifulSoup(page11.content)\n",
    "\n",
    "heading=[]\n",
    "for i in soup_coreym.find_all('h2',class_=\"entry-title\"):\n",
    "    heading.append(i.text)\n",
    "heading\n",
    "\n",
    "dates=[]\n",
    "for i in soup_coreym('time',class_=\"entry-time\"):\n",
    "    dates.append(i.text)\n",
    "dates\n",
    "\n",
    "content=[]\n",
    "for i in soup_coreym('div',class_=\"entry-content\"):\n",
    "    content.append(i.text)\n",
    "content\n",
    "\n",
    "i=soup_coreym.findAll('video',class_=\"video-stream html5-main-video\")\n",
    "for j in i:\n",
    "    video_url = i.find(\"a\")['href']\n",
    "    video_url\n",
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e6a9e",
   "metadata": {},
   "source": [
    "# Question 7-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "page12=requests.get(\"https://www.nobroker.in/property/sale/bangalore/Electronic%20City?type=BHK4&searchParam=W3sibGF0IjoxMi44N%20DUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8%20iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0=&propertyAge=0&radius=2.0\")\n",
    "page12\n",
    "\n",
    "soup_house=BeautifulSoup(page12.content)\n",
    "\n",
    "house_title=[]\n",
    "for i in soup_house.find_all('h2',class_=\"heading-6 font-semi-bold nb__25Cl7\"):\n",
    "    house_title.append(i.text)\n",
    "house_title\n",
    "\n",
    "location=[]\n",
    "for i in soup_house.find_all('div',class_=\"nb__nXU01\"):\n",
    "    location.append(i.text)\n",
    "print(\"=\"*10,\"Location\",\"=\"*10,\"\\n\",location)\n",
    "area=[]\n",
    "for i in soup_house.find_all('div',class_=\"nb__FfHqA\"):\n",
    "    area.append(i.text)\n",
    "print(\"=\"*10,\"Area\",\"=\"*10,\"\\n\",area)\n",
    "EMI=[]\n",
    "for i in soup_house.find_all('div',class_=\"font-semi-bold heading-6\",id=\"roomType\"):\n",
    "    EMI.append(i.text)\n",
    "print(\"=\"*10,\"EMI\",\"=\"*10,\"\\n\",EMI)\n",
    "price=[]\n",
    "for i in soup_house.find_all('div',class_=\"nb__4L90a\",id=\"minDeposit\"):\n",
    "        price.append(i.text)\n",
    "print(\"=\"*10,\"Price\",\"=\"*10,\"\\n\",price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5c50f",
   "metadata": {},
   "source": [
    "# Question 8-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "page13=requests.get(\"https://www.dineout.co.in/delhi-restaurants/dineout-pay\")\n",
    "page13\n",
    "\n",
    "soup_dine=BeautifulSoup(page13.content)\n",
    "Restaurant_name=[]\n",
    "for i in soup_dine.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "    Restaurant_name.append(i.text)\n",
    "print(\"=\"*30,\"Restaurant Name\",\"=\"*30,\"\\n\",Restaurant_name)\n",
    "Cuisine=[]\n",
    "for i in soup_dine.find_all('div',class_=\"detail-info\"):\n",
    "    Cuisine.append(i.text)\n",
    "print(\"=\"*30,\"Cuisine\",\"=\"*30,\"\\n\",Cuisine)\n",
    "Location=[]\n",
    "for i in soup_dine.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "    Location.append(i.text)\n",
    "print(\"=\"*30,\"Location\",\"=\"*30,\"\\n\",Location)\n",
    "Rating=[]\n",
    "for i in soup_dine.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "        Rating.append(i.text)\n",
    "print(\"=\"*30,\"Rating\",\"=\"*30,\"\\n\",Rating)\n",
    "image=[]\n",
    "for i in soup_dine.find_all('img',class_=\"no-img\"):\n",
    "    image.append(i['data-src'])\n",
    "print(\"=\"*30,\"Image URL\",\"=\"*30,\"\\n\",image)\n",
    "\n",
    "import pandas as pd\n",
    "dineout=pd.DataFrame({'Restarent Name':Restaurant_name,'Cuisine':Cuisine,\"Location\":Location,'Rating':Rating,\"Image URL\":image})\n",
    "dineout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81acb4a5",
   "metadata": {},
   "source": [
    "# Question 9-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e898a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "page14=requests.get(\"https://www.tutiempo.net/ahmadabad.html?datos=ultimas-24-horas\")\n",
    "page14\n",
    "\n",
    "soup_tutiem=BeautifulSoup(page14.content)\n",
    "hour=[]\n",
    "for i in soup_tutiem.find_all('font',{'style':\"vertical-align: inherit;\"}):\n",
    "    hour.append(i.text)\n",
    "print(\"=\"*30,\"Hours\",\"=\"*30,\"\\n\",hour)\n",
    "temp=[]\n",
    "for i in soup_tutiem.find_all('td',class_=\"t Temp\"):\n",
    "    temp.append(i.text)\n",
    "print(\"=\"*30,\"Temperature\",\"=\"*30,\"\\n\",temp)\n",
    "wind=[]\n",
    "for i in soup_tutiem.find_all('td',class_=\"wind\"):\n",
    "    wind.append(i.text)\n",
    "print(\"=\"*30,\"Wind\",\"=\"*30,\"\\n\",wind)\n",
    "humidity=[]\n",
    "for i in soup_tutiem.find_all('td',class_=\"hr\"):\n",
    "    humidity.append(i.text)\n",
    "print(\"=\"*30,\"Humidity\",\"=\"*30,\"\\n\",humidity)\n",
    "pressure=[]\n",
    "for i in soup_tutiem.find_all('td',class_=\"prob\"):\n",
    "    pressure.append(i.text)\n",
    "print(\"=\"*30,\"Pressure\",\"=\"*30,\"\\n\",pressure)\n",
    "\n",
    "import pandas as pd\n",
    "whether=pd.DataFrame({'Temperature':temp,'Wind':wind,'Humidity':humidity,'Pressure':pressure})\n",
    "whether"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f73e4b",
   "metadata": {},
   "source": [
    "# Question 10-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "page15=requests.get(\"https://www.puredestinations.co.uk/category/destinations/\")\n",
    "page15\n",
    "\n",
    "soup_destination=BeautifulSoup(page15.content)\n",
    "name=[]\n",
    "for i in soup_destination.find_all('h4',class_=\"title title--base u-remove-margin\"):\n",
    "    name.append(i.text)\n",
    "print(\"=\"*30,\"Monument Name\",\"=\"*30,\"\\n\",name)\n",
    "imagee=[]\n",
    "for i in soup_destination.find_all('img',class_=\"grid--image ls-is-cached lazyloaded\"):\n",
    "    imagee.append(i['data-src'])\n",
    "print(\"=\"*30,\"Image URL\",\"=\"*30,\"\\n\",imagee)\n",
    "desc=[]\n",
    "for i in soup_destination.find_all('div',class_=\"grid--item__content u-align-left\"):\n",
    "    desc.append(i.text)\n",
    "desc\n",
    "\n",
    "import pandas as pd\n",
    "monuments=pd.DataFrame({'Monument Name':name,'Description':desc})\n",
    "monuments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
