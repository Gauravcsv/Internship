{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a2d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1-: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps: \n",
    "\n",
    "1-First get the webpage https://www.naukri.com/\n",
    "2-Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3-Then click the search button.\n",
    "4-Then scrape the data for the first 10 jobs results you get.\n",
    "5-Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "549ff9d7",
   "metadata": {},
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d07fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "\n",
    "search_loc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()\n",
    "\n",
    "url1=\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\"\n",
    "\n",
    "driver.get(url1)\n",
    "\n",
    "\n",
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations_list=[]\n",
    "experiences_list=[]\n",
    "\n",
    "\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:4]\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:4]\n",
    "\n",
    "\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:4]\n",
    "\n",
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:4]\n",
    "\n",
    "\n",
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:4]\n",
    "\n",
    "for i in locations_tags:\n",
    "    locations=i.text\n",
    "    locations_list.append(locations)\n",
    "locations_list[0:4]\n",
    "\n",
    "\n",
    "experiences_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "experiences_tags[0:4]\n",
    "\n",
    "for i in experiences_tags:\n",
    "    experience=i.text\n",
    "    experiences_list.append(experience)\n",
    "experiences_list[0:4]\n",
    "\n",
    "\n",
    "(len(job_titles),len(company_names),len(locations_list),len(experiences_list))\n",
    "\n",
    "\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Job_title']=job_titles[0:10]\n",
    "jobs['Company_name']=company_names[0:10]\n",
    "jobs['Job_location']=locations_list[0:10]\n",
    "jobs['Experience_required']=experiences_list[0:10]\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "    \n",
    "1. First get the webpage https://www.naukri.com/ 2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f056e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-e938421603f2>:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
      "<ipython-input-3-e938421603f2>:10: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
      "<ipython-input-3-e938421603f2>:12: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  search_loc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
      "<ipython-input-3-e938421603f2>:26: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
      "<ipython-input-3-e938421603f2>:34: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
      "<ipython-input-3-e938421603f2>:44: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Snaphunt</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science - Senior Data Scientist</td>\n",
       "      <td>Paytm</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead / Principal Data Scientist</td>\n",
       "      <td>Aviso Inc</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Ally-Executive</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Visa</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Credit Risk</td>\n",
       "      <td>Scienaptic Systems</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lead/Senior Data Scientist (NLP)</td>\n",
       "      <td>Samya.AI A FRACTAL Entity</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Truecaller</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Slice</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Advanced Analytics Expert - Data Scientist</td>\n",
       "      <td>Hewlett-Packard</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    job_title               company_name  \\\n",
       "0                              Data Scientist                   Snaphunt   \n",
       "1        Data Science - Senior Data Scientist                      Paytm   \n",
       "2             Lead / Principal Data Scientist                  Aviso Inc   \n",
       "3                       Senior Data Scientist             Ally-Executive   \n",
       "4                       Senior Data Scientist                       Visa   \n",
       "5                Data Scientist - Credit Risk         Scienaptic Systems   \n",
       "6            Lead/Senior Data Scientist (NLP)  Samya.AI A FRACTAL Entity   \n",
       "7                              Data Scientist                 Truecaller   \n",
       "8                              Data Scientist                      Slice   \n",
       "9  Advanced Analytics Expert - Data Scientist            Hewlett-Packard   \n",
       "\n",
       "                 job_location  \n",
       "0         Bangalore/Bengaluru  \n",
       "1  Noida, Bangalore/Bengaluru  \n",
       "2                              \n",
       "3                              \n",
       "4                              \n",
       "5                              \n",
       "6                              \n",
       "7                              \n",
       "8                              \n",
       "9                              "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "search_loc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Bangalore/Bengaluru\")\n",
    "\n",
    "\n",
    "\n",
    "url2=\"https://www.naukri.com/data-scientist-jobs-in-bangalore-bengaluru?k=data%20scientist&l=bangalore%2Fbengaluru\"\n",
    "\n",
    "driver.get(url2)\n",
    "\n",
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations_list=[]\n",
    "\n",
    "\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:4]\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:4]\n",
    "\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:4]\n",
    "\n",
    "\n",
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:4]\n",
    "\n",
    "\n",
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:4]\n",
    "\n",
    "for i in locations_tags:\n",
    "    locations=i.text\n",
    "    locations_list.append(locations)\n",
    "locations_list[0:4]\n",
    "\n",
    "(len(job_titles),len(company_names),len(locations_list))\n",
    "\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job_title']=job_titles[0:10]\n",
    "jobs['company_name']=company_names[0:10]\n",
    "jobs['job_location']=locations_list[0:10]\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3-: In this question you have to scrape data using the filters available on the webpage as shown below: You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is “Delhi/NCR” The salary filter to be used is “3-6” lakhs The task will be done as shown in the below steps:\n",
    "\n",
    "1-first get the webpage https://www.naukri.com/\n",
    "2-Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3-Then click the search button.\n",
    "4-Then apply the location filter and salary filter by checking the respective boxes\n",
    "5-Then scrape the data for the first 10 jobs results you get.\n",
    "6-Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c717c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_loc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Delhi\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()\n",
    "\n",
    "select_location = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[2]/label/p/span[1]')\n",
    "select_location.click()\n",
    "\n",
    "select_salary = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]/label/p/span[1]')\n",
    "select_salary.click()\n",
    "\n",
    "url3=\"https://www.naukri.com/data-scientist-jobs-in-delhi?k=data%20scientist&l=delhi&cityTypeGid=9508&ctcFilter=3to6\"\n",
    "\n",
    "driver.get(url3)\n",
    "\n",
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations_list=[]\n",
    "\n",
    "\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:4]\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:4]\n",
    "\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:4]\n",
    "\n",
    "\n",
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:4]\n",
    "\n",
    "\n",
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:4]\n",
    "\n",
    "for i in locations_tags:\n",
    "    locations=i.text\n",
    "    locations_list.append(locations)\n",
    "locations_list[0:4]\n",
    "\n",
    "(len(job_titles),len(company_names),len(locations_list))\n",
    "\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['titles']=job_titles[0:10]\n",
    "jobs['company']=company_names[0:10]\n",
    "jobs['location']=locations_list[0:10]\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quation 4-  Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "1-Brand\n",
    "2-Product Description\n",
    "3-Price\n",
    "4-Discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4aaf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "\n",
    "login_pop = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "login_pop.click()\n",
    "\n",
    "search_job=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "search_job.send_keys(\"sunglasses\")\n",
    "search_job.submit()\n",
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "    \n",
    "Brand_name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_name.append(i.text)\n",
    "    \n",
    "try:\n",
    "    description_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "except NoSuchElementException as e:\n",
    "    description_tags=' '\n",
    "    \n",
    "Product_Description=[]\n",
    "for i in description_tags:\n",
    "    Product_Description.append(i.text)\n",
    "    \n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "    \n",
    "Product_Price=[]\n",
    "for i in price_tags:\n",
    "    Product_Price.append(i.text)\n",
    "    \n",
    "import pandas as pd\n",
    "Sunglasses=pd.DataFrame({})\n",
    "Sunglasses['Brand']=Brand_name[0:100]\n",
    "Sunglasses['Description']=Product_Description[0:100]\n",
    "Sunglasses['Price']=Product_Price[0:100]\n",
    "Sunglasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5- Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC\n",
    "TSVZAXUHGREPBFGI&marketplace\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33286cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "\n",
    "for page in range(1, 11, 1):\n",
    "    driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "    url='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART'    \n",
    "    driver.get(url)\n",
    "    try:\n",
    "        rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    except NoSuchElementException as e:\n",
    "        rating_tags='-'\n",
    "        \n",
    "    Ratings=[]\n",
    "    for i in rating_tags:\n",
    "        Ratings.append(i.text)\n",
    "    \n",
    "    try:\n",
    "        summary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    except NoSuchElementException as e:\n",
    "        summary_tags='-'\n",
    "        \n",
    "    Summary=[]\n",
    "    for i in summary_tags:\n",
    "        Summary.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        FullReview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']//div//div\")\n",
    "    except NoSuchElementException as e:\n",
    "        FullReview_tags='-'\n",
    "        \n",
    "    FullSummary=[]\n",
    "    for i in FullReview_tags:\n",
    "        FullSummary.append(i.text)\n",
    "\n",
    "import pandas as pd\n",
    "IPhone=pd.DataFrame({})\n",
    "IPhone['Ratings']=Ratings[0:100]\n",
    "IPhone['Summary']=Summary[0:100]\n",
    "IPhone['FullSummary']=FullSummary[0:100]\n",
    "IPhone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7237c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 6-: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfc642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "\n",
    "login_pop = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "# Here .click function use to tap on desire elements of webpage\n",
    "login_pop.click()\n",
    "\n",
    "\n",
    "search_job=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "search_job.send_keys(\"sneakers\")\n",
    "search_job.submit()\n",
    "\n",
    "\n",
    "for page in range(1, 11, 1):\n",
    "    driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    url='https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    except NoSuchElementException as e:\n",
    "        brand_tags='-'\n",
    "        \n",
    "    BrandTags=[]\n",
    "    for i in brand_tags:\n",
    "        BrandTags.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        ProductDesc_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    except NoSuchElementException as e:\n",
    "        ProductDesc_tags='-'\n",
    "        \n",
    "    ProductDesc=[]\n",
    "    for i in ProductDesc_tags:\n",
    "        ProductDesc.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    except NoSuchElementException as e:\n",
    "        Price_tags='-'\n",
    "        \n",
    "    Price=[]\n",
    "    for i in Price_tags:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        Discount_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "    except NoSuchElementException as e:\n",
    "        Discount_tags='-'\n",
    "        \n",
    "    Discount=[]\n",
    "    for i in Discount_tags:\n",
    "        Discount.append(i.text)\n",
    "        \n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "sneakers=pd.DataFrame({})\n",
    "sneakers['Brand']=BrandTags[0:30]\n",
    "sneakers['Shoe Description']=ProductDesc[0:30]\n",
    "sneakers['Price']=Price[0:30]\n",
    "sneakers['Discount']=Discount[0:30]\n",
    "sneakers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quesion 7-: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d507daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)\n",
    "price=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\")\n",
    "price.click()\n",
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "        \n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "        \n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "        \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand']=Brand_Name[0:10]\n",
    "shoes['Shoe Description']=Product_Desc[0:10]\n",
    "shoes['Price']=Prices[0:10]\n",
    "shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Color filter to “Black”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6093aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)\n",
    "color=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label\")\n",
    "color.click()\n",
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "        \n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "        \n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "        \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand']=Brand_Name[0:10]\n",
    "shoes['Shoe Description']=Product_Desc[0:10]\n",
    "shoes['Price']=Prices[0:10]\n",
    "shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8-: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73428bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "Url=\"https://www.amazon.in/\"\n",
    "\n",
    " \n",
    "driver.get(Url)\n",
    "\n",
    "\n",
    "inputboxs = driver.find_elements(By.CLASS_NAME,\"nav-search-field \")\n",
    "print(len(inputboxs))\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\").send_keys(\"Laptop\")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\").click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "Intel_Core_i7 = driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[1]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[2]/li[16]/span\")\n",
    "try:\n",
    "    Intel_Core_i7.click()\n",
    "except ElementNotInteractableException:\n",
    "    drive.get(Intel_Core_i7.get_attribute(\"herf\"))\n",
    "\n",
    "time.sleep(4) \n",
    "\n",
    "Intel_Core_i9 = driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[1]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[1]/li[18]/span/a/span\")\n",
    "try:\n",
    "    Intel_Core_i9.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(Intel_Core_i9.get_attribute(\"herf\"))\n",
    "    \n",
    "time.sleep(3)\n",
    "\n",
    "Laptop_Brand = [] \n",
    "Ratings = []\n",
    "Price = []\n",
    "import time\n",
    "\n",
    "for j in range(0,5,1):\n",
    "    Laptop_Brand_tag=driver.find_elements_by_xpath('//span[@class=\"a-size-medium a-color-base a-text-normal\"]') \n",
    "    for i in Laptop_Brand_tag:\n",
    "        Laptop_Brand.append(i.text)\n",
    "          \n",
    "    Ratings_tag=driver.find_elements_by_xpath('//span[@class=\"a-icon-alt\"]') \n",
    "    for i in Ratings_tag:\n",
    "        Ratings.append(i.text)\n",
    "        \n",
    "        \n",
    "    Price_tag=driver.find_elements_by_xpath('//span[@class=\"a-price-whole\"]') \n",
    "    for i in Price_tag:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "        \n",
    "    driver.find_element_by_xpath('//ul[@class=\"a-pagination\"]').click()\n",
    "    \n",
    "    time.sleep(4)\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "laptop_ws = pd.DataFrame({})\n",
    "laptop_ws[\"Title\"]=Laptop_Brand[0:50]\n",
    "laptop_ws[\"Ratings\"]=Ratings[0:50]\n",
    "laptop_ws[\"Price\"]=Price[0:50]\n",
    "\n",
    "\n",
    "\n",
    "laptop_ws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9-: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25498415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "driver9= webdriver.Chrome(r\"D:/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "driver9.get(\"https://www.ambitionbox.com/\")\n",
    "jobs= driver9.find_element_by_xpath('//*[@id=\"headerWrapper\"]/nav/nav/a[6]')\n",
    "jobs.click()\n",
    "\n",
    "search= driver9.find_element_by_xpath('//*[@id=\"jobs-typeahead\"]/span/input')\n",
    "search.send_keys('data scientist')\n",
    "\n",
    "search_btn= driver9.find_element_by_xpath('//*[@id=\"jobSearchPage\"]/div[2]/div[1]/div/div/div/button')\n",
    "search_btn.click()\n",
    "\n",
    "location= driver9.find_element_by_xpath('//*[@id=\"filters-row\"]/div/div/div[2]/div[1]/i').click()\n",
    "\n",
    "search_location= driver9.find_element_by_xpath('//*[@id=\"filters-row\"]/div/div/div[2]/div[2]/div/div[2]/input')\n",
    "search_location.send_keys(\"Noida\")\n",
    "\n",
    "noida_location= driver9.find_element_by_xpath('//*[@id=\"filters-row\"]/div/div/div[2]/div[2]/div/div[3]/div/div/div[1]/label')\n",
    "noida_location.click()\n",
    "\n",
    "company_name=[]\n",
    "time=[]\n",
    "rating=[]\n",
    "\n",
    "company = driver9.find_elements_by_xpath('//div[@class=\"company-info\"]')\n",
    "for i in company:\n",
    "    company_name.append(i.text)\n",
    "company_name[0:3]\n",
    "\n",
    "timing= driver9.find_elements_by_xpath('//span[@class=\"body-small-l\"]')\n",
    "for i in timing:\n",
    "    time.append(i.text)\n",
    "time[0:3]\n",
    "\n",
    "rate= driver9.find_elements_by_xpath('//span[@class=\"body-small\"]')\n",
    "for i in rate:\n",
    "    rating.append(i.text)\n",
    "rating[0:3]\n",
    "\n",
    "ambitionbox_data=pd.DataFrame({})\n",
    "ambitionbox_data[\"Comany_name\"]=company_name[0:10]\n",
    "ambitionbox_data[\"Posted_Before\"]=time[0:10]\n",
    "ambitionbox_data[\"Rating\"]=rating[0:10]\n",
    "ambitionbox_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 10-: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver10= webdriver.Chrome(r\"D:/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "driver10.get(\"https://www.ambitionbox.com/\")\n",
    "\n",
    "salaries_tab = driver10.find_element_by_xpath('//*[@id=\"headerWrapper\"]/nav/nav/a[4]')\n",
    "salaries_tab.click()\n",
    "\n",
    "search_job_profile = driver10.find_element_by_xpath('//*[@id=\"jobProfileSearchbox\"]')\n",
    "search_job_profile.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_datascientist= driver10.find_element_by_xpath('/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div/div[1]')\n",
    "search_datascientist.click()\n",
    "\n",
    "company_name=[]\n",
    "salary=[]\n",
    "avg_salary=[]\n",
    "min_salary=[]\n",
    "max_salary=[]\n",
    "exp_required=[]\n",
    "company= driver10.find_elements_by_xpath('//div[@class=\"name\"]/a')\n",
    "for i in company:\n",
    "    company_name.append(i.text)\n",
    "    \n",
    "company_name\n",
    "\n",
    "salaries= driver10.find_elements_by_xpath('//div[@class=\"name\"]/span')\n",
    "for i in salaries:\n",
    "    salary.append(i.text)\n",
    "salary\n",
    "\n",
    "avg= driver10.find_elements_by_xpath('//div[@class=\"average-indicator-wrapper\"]')\n",
    "for i in avg:\n",
    "    avg_salary.append(i.text)\n",
    "avg_salary[0:10]\n",
    "\n",
    "min= driver10.find_elements_by_xpath('//div[@class=\"value body-medium\"]')\n",
    "for i in range(0,len(min),2):\n",
    "    min_salary.append(min[i].text)\n",
    "min_salary[0:10]\n",
    "\n",
    "max= driver10.find_elements_by_xpath('//div[@class=\"value body-medium\"]')\n",
    "for i in range(1,len(max),2):\n",
    "    max_salary.append(max[i].text)\n",
    "max_salary[0:10]\n",
    "\n",
    "exp= driver10.find_elements_by_xpath('//div[@class=\"salaries sbold-list-header\"]')\n",
    "for i in exp:\n",
    "    exp_required.append(i.text.replace(\"\\n\",\"\"))\n",
    "exp_required[0:10]\n",
    "\n",
    "print(len(company_name),len(avg_salary),len(min_salary),len(max_salary),len(exp_required))\n",
    "\n",
    "ambitionbox_dataset=pd.DataFrame({})\n",
    "ambitionbox_dataset[\"Company_Name\"]=company_name[0:10]\n",
    "ambitionbox_dataset[\"Total_Salary\"]=salary[0:10]\n",
    "ambitionbox_dataset[\"Avg_Salary\"]=avg_salary[0:10]\n",
    "ambitionbox_dataset[\"Min_Salary\"]=min_salary[0:10]\n",
    "ambitionbox_dataset[\"Max_Salary\"]=max_salary[0:10]\n",
    "ambitionbox_dataset[\"Total_Experience\"]=exp_required[0:10]\n",
    "ambitionbox_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d2d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
