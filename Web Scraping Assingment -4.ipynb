{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2deb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba33ee",
   "metadata": {},
   "source": [
    "# Question 1-"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e09f1cd4",
   "metadata": {},
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get('https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos')\n",
    "\n",
    "Rank=[]\n",
    "Name=[]\n",
    "Artist=[]\n",
    "Upload_date=[]\n",
    "Views=[]\n",
    "\n",
    "for b in driver.find_elements_by_xpath(\"//td[@allign='center']\"):\n",
    "    Rank.append(b.text)\n",
    "for c in driver.find_elements_by_xpath(\"//th[@allign='headerSort']\"):\n",
    "    Name.append(b.text)\n",
    "for d in driver.find_elements_by_xpath(\"//th[@allign='headerSort']\"):\n",
    "    Artist.append(b.text)\n",
    "for e in driver.find_elements_by_xpath(\"//th[@allign='headerSort']\"):\n",
    "    Upload_date.append(b.text)\n",
    "for f in driver.find_elements_by_xpath(\"//th[@allign='headerSort']\"):\n",
    "    \n",
    "    Views.append(b.text)\n",
    "    \n",
    "Most_viewed_video=pd.DataFrame({})\n",
    "Most_viewed_video[\"RANK\"]= Rank\n",
    "Most_viewed_video[\"NAME\"]= Name\n",
    "Most_viewed_video[\"ARTIST\"]= Artist\n",
    "Most_viewed_video[\"UPLOAD DATE\"]= Upload_date\n",
    "Most_viewed_video[\"VIEWS\"]= Views\n",
    "Most_viewed_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625bf214",
   "metadata": {},
   "source": [
    "# Question 2-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import *\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.bcci.tv/\")\n",
    "\n",
    "action = ActionChains(driver)\n",
    "\n",
    "menu = driver.find_element_by_xpath(\"//div[@class='navigation__drop-down drop-down drop-down--reveal-on-hover']\")\n",
    "action.move_to_element(menu).perform()\n",
    "fixture = driver.find_element_by_xpath(\"//div[@class='navigation__drop-down drop-down drop-down--reveal-on-hover']/div/ul/li[1]\")\n",
    "fixture.click()\n",
    "time.sleep(4)\n",
    "try:\n",
    "    title = driver.find_elements_by_xpath(\"//div[@class='fixture__description u-unskewed-text']/p/strong\")\n",
    "    series = driver.find_elements_by_xpath(\"//div[@class='fixture__format-strip']/span[2]\")\n",
    "    place = driver.find_elements_by_xpath(\"//div[@class='fixture__description u-unskewed-text']/p/span\")\n",
    "    month =  driver.find_elements_by_xpath(\"//div[@class='fixture__date-details']/span[1]\")\n",
    "    date = driver.find_elements_by_xpath(\"//span[@class='fixture__date']\")\n",
    "    time = driver.find_elements_by_xpath(\"//div[@class='fixture__date-details']/span[2]\")\n",
    "except:\n",
    "    pass\n",
    "Title = []\n",
    "Series = []\n",
    "Place  =[]\n",
    "Month = []\n",
    "Date = []\n",
    "Time = []\n",
    "for i in title:\n",
    "    Title.append(i.text)\n",
    "for i in series:\n",
    "    Series.append(i.text)\n",
    "for i in place:\n",
    "    Place.append(i.text)\n",
    "for i in month:\n",
    "    Month.append(i.text)\n",
    "for i in date:\n",
    "    Date.append(i.text)\n",
    "for i in time:\n",
    "    Time.append(i.text)\n",
    "data  = {\n",
    "    'Match Title': Title,\n",
    "    'Series Name': Series,\n",
    "    'Place':Place,\n",
    "    'Month': Month,\n",
    "    'Date':Date,\n",
    "    'Time':Time\n",
    "}\n",
    "IndiasMatches = pd.DataFrame.from_dict(data, orient='index').transpose()\n",
    "IndiasMatches.to_csv(\"IndiasMatches.csv\", encoding='utf-8', header='''India's International Matches''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de3b5fe",
   "metadata": {},
   "source": [
    "# Question 3-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import *\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9530cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.guru99.com/\")\n",
    "\n",
    "sele = driver.find_element_by_xpath(\"//ul[@id='java_technologies']/li[3]\")\n",
    "sele.click()\n",
    "\n",
    "exception = driver.find_element_by_xpath(\"//table[@class='table'][5]/tbody/tr[34]/td/a\")\n",
    "exception.click()\n",
    "try:\n",
    "    name = driver.find_elements_by_xpath(\"//table[@class='table table-striped']/tbody/tr/td[1]\")\n",
    "    description = driver.find_elements_by_xpath(\"//table[@class='table table-striped']/tbody/tr/td[2]\")\n",
    "except:\n",
    "    pass\n",
    "Name = []\n",
    "Description = []\n",
    "for i in name:\n",
    "    Name.append(i.text)\n",
    "for i in description:\n",
    "    Description.append(i.text)\n",
    "data = {\n",
    "    'Name': Name[1:],\n",
    "    'Description': Description[1:]\n",
    "}\n",
    "SeleniumExceptions = pd.DataFrame.from_dict(data)\n",
    "SeleniumExceptions.to_csv(\"Selenium Exceptions\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116abc01",
   "metadata": {},
   "source": [
    "# Question 4-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f5239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import *\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "\n",
    "btn = driver.find_element_by_xpath(\"//div[@class='dropdown'][2]\").click()\n",
    "india  = driver.find_element_by_xpath(\"//div[@class='dropdown'][2]/div/a[3]\").click()\n",
    "gdp  = driver.find_element_by_xpath(\"//body/div[2]/div[2]/div[2]/ul/li[2]\").click()\n",
    "try:\n",
    "    rank = driver.find_elements_by_xpath(\"//table[@id='table_id']/tbody/tr/td[1]\")\n",
    "    state = driver.find_elements_by_xpath(\"//table[@id='table_id']/tbody/tr/td[2]\")\n",
    "    n =  driver.find_elements_by_xpath(\"//table[@id='table_id']/tbody/tr/td[3]\")\n",
    "    e = driver.find_elements_by_xpath(\"//table[@id='table_id']/tbody/tr/td[4]\")\n",
    "    p = driver.find_elements_by_xpath(\"//table[@id='table_id']/tbody/tr/td[5]\")\n",
    "    gdp = driver.find_elements_by_xpath(\"//table[@id='table_id']/tbody/tr/td[6]\")\n",
    "    ni = driver.find_elements_by_xpath(\"//table[@id='table_id']/tbody/tr/td[7]\")\n",
    "    ei = driver.find_elements_by_xpath(\"//table[@id='table_id']/tbody/tr/td[8]\")\n",
    "except:\n",
    "    pass\n",
    "Rank = []\n",
    "State = []\n",
    "N = []\n",
    "E = []\n",
    "P = []\n",
    "GDP = []\n",
    "NI = []\n",
    "EI = []\n",
    "for i in rank:\n",
    "    Rank.append(i.text)\n",
    "for j in state:\n",
    "    State.append(j.text)\n",
    "for k in n:\n",
    "    N.append(k.text)\n",
    "for l in e:\n",
    "    E.append(l.text)\n",
    "for m in gdp:\n",
    "    GDP.append(m.text)\n",
    "for n in ni:\n",
    "    NI.append(n.text)\n",
    "for o in ei:\n",
    "    EI.append(o.text)\n",
    "for p in p:\n",
    "    P.append(p.text)\n",
    "data = {\n",
    "    'Rank': Rank,\n",
    "    'State Name': State,\n",
    "    '19-20': N,\n",
    "    '18-19':E,\n",
    "    'GDP % of india': P,\n",
    "    'GDP per capita': GDP,\n",
    "    'GSDP 19-20':NI,\n",
    "    'GSDP 18-19':EI\n",
    "}\n",
    "StateWiseGDPIndia = pd.DataFrame.from_dict(data)\n",
    "StateWiseGDPIndia.to_csv('StateWiseGDPIndia.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33bed7",
   "metadata": {},
   "source": [
    "# Question 5-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import *\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://github.com/\")\n",
    "\n",
    "signin = driver.find_element_by_xpath(\"//a[@class='HeaderMenu-link flex-shrink-0 no-underline mr-3']\")\n",
    "signin.click()\n",
    "username = driver.find_element_by_xpath(\"//input[@id='login_field']\")\n",
    "user = input(\"Enter Your Email address: \")\n",
    "username.send_keys(user)\n",
    "password = driver.find_element_by_xpath(\"//input[@id='password']\")\n",
    "pwd = input(\"Enter your password: \")\n",
    "password.send_keys(pwd)\n",
    "btn = driver.find_element_by_xpath(\"//input[@class='btn btn-primary btn-block']\")\n",
    "btn.click()\n",
    "\n",
    "otp = driver.find_element_by_xpath(\"//input[@id='otp']\")\n",
    "otp.clear()\n",
    "OTP = input(\"Please Enter the OTP Received by You: \")\n",
    "otp.send_keys(OTP)\n",
    "btn = driver.find_element_by_xpath(\"//button[@class='btn btn-primary btn-block']\")\n",
    "btn.click()\n",
    "\n",
    "explore  = driver.find_element_by_xpath(\"//a[@class='js-selected-navigation-item Header-link mt-md-n3 mb-md-n3 py-2 py-md-3 mr-0 mr-md-3 border-top border-md-top-0 border-white-fade'][3]\")\n",
    "explore.click()\n",
    "trend = driver.find_element_by_xpath(\"//a[@class='js-selected-navigation-item d-inline-block py-2 py-md-3 mr-3 mr-md-4 no-underline subnav-link'][2]\")\n",
    "trend.click()\n",
    "try:\n",
    "    title = driver.find_elements_by_xpath(\"//h1[@class='h3 lh-condensed']/a\")\n",
    "    desc = driver.find_elements_by_xpath(\"//p[@class='col-9 color-text-secondary my-1 pr-4']\")\n",
    "    lang = driver.find_elements_by_xpath(\"//span[@class='d-inline-block ml-0 mr-3']/span[2]\")\n",
    "    stars = driver.find_elements_by_xpath(\"//div[@class='f6 color-text-secondary mt-2']/a[1]\")\n",
    "    forks = driver.find_elements_by_xpath(\"//div[@class='f6 color-text-secondary mt-2']/a[2]\")\n",
    "except:\n",
    "    pass\n",
    "Title = []\n",
    "Description = []\n",
    "Language = []\n",
    "Stars  =[]\n",
    "Forks = []\n",
    "for i in title:\n",
    "    Title.append(i.text)\n",
    "for j in desc:\n",
    "    Description.append(j.text)\n",
    "for k in lang:\n",
    "    Language.append(k.text)\n",
    "for l in stars:\n",
    "    Stars.append(l.text)\n",
    "for m in forks:\n",
    "    Forks.append(m.text)\n",
    "data = {\n",
    "    'Repository Title' : Title,\n",
    "    'Repository Description' : Description,\n",
    "    'Language Used' : Language,\n",
    "    'Stars' : Stars,\n",
    "    'Forks' : Forks\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5400ce",
   "metadata": {},
   "source": [
    "# Question 6-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import *\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75035398",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.billboard.com/\")\n",
    "hot100 = driver.find_element_by_xpath(\"//nav[@class='header__subnav bg--light']/ul/li[3]\")\n",
    "hot100.click()\n",
    "try:\n",
    "    name = driver.find_elements_by_xpath(\"//span[@class='chart-element__information']/span[1]\")\n",
    "    artist = driver.find_elements_by_xpath(\"//span[@class='chart-element__information']/span[2]\")\n",
    "    lwr = driver.find_elements_by_xpath(\"//div[@class='chart-element__metas chart-element__metas--large display--flex flex--y-center']/div[2]\")\n",
    "    pr = driver.find_elements_by_xpath(\"//div[@class='chart-element__metas chart-element__metas--large display--flex flex--y-center']/div[3]\")\n",
    "    weeks = driver.find_elements_by_xpath(\"//div[@class='chart-element__metas chart-element__metas--large display--flex flex--y-center']/div[4]\")\n",
    "except:\n",
    "    pass\n",
    "Name = []\n",
    "Artist = []\n",
    "Last = []\n",
    "Peak = []\n",
    "Weeks = []\n",
    "for i in name:\n",
    "    Name.append(i.text)\n",
    "for j in artist:\n",
    "    Artist.append(i.text)\n",
    "for k in lwr:\n",
    "    Last.append(k.text)\n",
    "for l in pr:\n",
    "    Peak.append(l.text)\n",
    "for m in weeks:\n",
    "    Weeks.append(m.text)\n",
    "data = {\n",
    "    'Song Name' : Name,\n",
    "    'Artist Name' : Artist,\n",
    "    'Last Week Rank' : Last,\n",
    "    'Peak Rank': Peak,\n",
    "    'Weeks on board' : Weeks\n",
    "}\n",
    "Hot100 = pd.DataFrame.from_dict(data, orient='index').transpose()\n",
    "Hot100.fillna(\"-\", inplace =True)\n",
    "Hot100.to_csv(\"Hot100 Songs.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c652826",
   "metadata": {},
   "source": [
    "# Question 7-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799692eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "search_button=driver.find_element_by_xpath(\"//a[@title='Search Recruiters']\")\n",
    "search_button.click()\n",
    "Name=[]\n",
    "Designation=[]\n",
    "Company=[]\n",
    "Skills=[]\n",
    "Location=[]\n",
    "\n",
    "for b in driver.find_elements_by_xpath(\"//span[@class='fl ellipsis']\"):\n",
    "    Name.append(b.text)\n",
    "for c in driver.find_elements_by_xpath(\"//span[@class='ellipsis clr']\"):\n",
    "    Designation.append(c.text)\n",
    "for d in driver.find_elements_by_xpath(\"//span[@class='ellipsis']\"):\n",
    "    Company.append(d.text)\n",
    "for e in driver.find_elements_by_xpath(\"//div[@class='hireSec']\"):\n",
    "    \n",
    "    Skills.append(e.text)\n",
    "\n",
    "    \n",
    "recruiters=pd.DataFrame({})\n",
    "recruiters[\"NAME\"]= Name[0:10]\n",
    "recruiters[\"Designation\"]=Designation\n",
    "recruiters[\"Company\"]=Company\n",
    "recruiters[\"Skills\"]=Skills\n",
    "recruiters[\"Location\"]=Location\n",
    "recruiters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c0841",
   "metadata": {},
   "source": [
    "# Question 8-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a276517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "def bookpage():\n",
    "    response = requests.get('https://bookpage.com/reviews/')\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    url_tags = soup.find_all('div', attrs = {'class': 'row-fluid article-row'})\n",
    "    urls = [i.find('h4').find_all('a')[0]['href'] for i in url_tags[0:5]]\n",
    "    book_dict = {}\n",
    "    book_dict[\"Book Name\"] = []\n",
    "    book_dict[\"Author\"] = []\n",
    "    book_dict[\"Genre\"] = []\n",
    "    book_dict[\"Review\"] = []\n",
    "    for url in urls:\n",
    "        book = requests.get('https://www.bookpage.com'+url)\n",
    "        soup = BeautifulSoup(book.content, 'html.parser')\n",
    "        book_dict[\"Book Name\"].append(soup.find('h1').text.replace('\\n',''))\n",
    "        book_dict[\"Author\"].append(soup.find('h4').text.replace('\\n',''))\n",
    "        book_dict[\"Genre\"].append(soup.find('p', attrs = {'class':'genre-links'}).text.replace('\\n',''))\n",
    "        book_dict[\"Review\"].append(soup.find('div', attrs = {'class':'article-body'}).text.replace('\\n',''))\n",
    "    book_df = pd.DataFrame.from_dict(book_dict)\n",
    "    book_df.to_csv('Book Reviews.csv', index = False)\n",
    "    return book_df\n",
    "\n",
    "bookpage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d849e52",
   "metadata": {},
   "source": [
    "# Question 9-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c8049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  names   years  ratings\n",
      "0              Jai Bhim  (2021)      8.5\n",
      "1     Pariyerum Perumal  (2018)      8.5\n",
      "2               Nayakan  (1987)      8.5\n",
      "3            Anbe Sivam  (2003)      8.5\n",
      "4     C/o Kancharapalem  (2018)      8.5\n",
      "..                  ...     ...      ...\n",
      "95  Munna Bhai M.B.B.S.  (2003)      8.1\n",
      "96            Sarfarosh  (1999)      8.1\n",
      "97                 Roja  (1992)      8.1\n",
      "98                Queen  (2013)      8.1\n",
      "99       Dil Chahta Hai  (2001)      8.1\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "def Indian_IMDB_top_100(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    name=[]\n",
    "    year=[]\n",
    "    rating=[]\n",
    "    for i in soup.find_all('img'):\n",
    "        name.append(i.attrs['alt'])\n",
    "    for i in list(soup.find_all('span',attrs={\"secondaryInfo\"})):\n",
    "        year.append(i.text)\n",
    "    for i in soup.find_all('strong'):\n",
    "        rating.append(float(i.text))\n",
    "    df=pd.DataFrame({'names':name[:100],\n",
    "                     'years':year[:100],\n",
    "                     'ratings':rating[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('indian_IMDB_top_100.csv', index = False)\n",
    "\n",
    "Indian_IMDB_top_100(\"https://www.imdb.com/india/top-rated-indian-movies/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88569bc",
   "metadata": {},
   "source": [
    "# Question 10-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b37ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import *\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edcb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://archive.ics.uci.edu/ml/index.php\")\n",
    "\n",
    "click  = driver.find_element_by_xpath(\"//table[2]/tbody/tr/td/span/b\")\n",
    "\n",
    "click.click()\n",
    "\n",
    "try:\n",
    "    name = driver.find_elements_by_xpath(\"//table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[1]\")\n",
    "    datatype = driver.find_elements_by_xpath(\"//table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]\")\n",
    "    task = driver.find_elements_by_xpath(\"//table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]\")\n",
    "    attrobutetype = driver.find_elements_by_xpath(\"//table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]\")\n",
    "    instances = driver.find_elements_by_xpath(\"//table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]\")\n",
    "    noattribute = driver.find_elements_by_xpath(\"//table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[6]\")\n",
    "    year = driver.find_elements_by_xpath(\"//table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]\")\n",
    "except:\n",
    "    pass\n",
    "Name = []\n",
    "Datatype = []\n",
    "Task = []\n",
    "Attribute = []\n",
    "Instances =  []\n",
    "NoAttribute = []\n",
    "Year = []\n",
    "for i in name:\n",
    "    Name.append(i.text)\n",
    "for j in datatype:\n",
    "    Datatype.append(j.text)\n",
    "for k in task:\n",
    "    Task.append(k.text)\n",
    "for l in attrobutetype:\n",
    "    Attribute.append(l.text)\n",
    "for m in instances:\n",
    "    Instances.append(m.text)\n",
    "for n in noattribute:\n",
    "    NoAttribute.append(n.text)\n",
    "for o in year:\n",
    "    Year.append(o.text)\n",
    "data = {\n",
    "    'Dataset Name': Name,\n",
    "    'Data Type': Datatype,\n",
    "    'Task' : Task,\n",
    "    'Attribute Type': Attribute,\n",
    "    'No of Instances' : Instances,\n",
    "    'No of Attribute': NoAttribute,\n",
    "    'Year': Year\n",
    "}\n",
    "Datasets = pd.DataFrame.from_dict(data)\n",
    "Datasets[1:].to_csv(\"Datasets.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d91fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
